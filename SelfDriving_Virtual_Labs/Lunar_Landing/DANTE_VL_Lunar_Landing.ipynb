{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1wICzLWaMCe"
   },
   "source": [
    "\n",
    "################################################################################\n",
    "> # **Clone GitHub repository**\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7DyLCIraLwf",
    "outputId": "065690ac-7ce1-4162-b662-6311b5cd1412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start============================================================================================\n",
      "fatal: destination path 'DANTE' already exists and is not an empty directory.\n",
      "Done============================================================================================\n"
     ]
    }
   ],
   "source": [
    "################# Clone repository from github to colab session ################\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "run this section if you want to clone all the datasets, preTrained networks from the GitHub\n",
    "repository to this colab session\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"Start============================================================================================\")\n",
    "\n",
    "!git clone https://github.com/Bop2000/DANTE\n",
    "\n",
    "print(\"Done============================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmPq8GSGbdCY",
    "outputId": "4f6fc5c6-14c0-4959-ec6c-0a5f799c032f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/dante.py' -> './dante.py'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/obj_functions.py' -> './obj_functions.py'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/README.md' -> './README.md'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/run.py' -> './run.py'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/run.sh' -> './run.sh'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/CMA-ES-LunarLander100/result' -> './Source_Data/CMA-ES-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/DANTE-LunarLander100/result' -> './Source_Data/DANTE-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/DOO-LunarLander100/result' -> './Source_Data/DOO-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/DifferentialEvolution-LunarLander100/result' -> './Source_Data/DifferentialEvolution-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/DualAnnealing-LunarLander100/result' -> './Source_Data/DualAnnealing-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/MCMC-LunarLander100/result' -> './Source_Data/MCMC-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/Random-LunarLander100/result' -> './Source_Data/Random-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/SOO-LunarLander100/result' -> './Source_Data/SOO-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/Shiwa-LunarLander100/result' -> './Source_Data/Shiwa-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/VOO-LunarLander100/result' -> './Source_Data/VOO-LunarLander100/result'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/__pycache__/doo.cpython-38.pyc' -> './Source_Data/__pycache__/doo.cpython-38.pyc'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/__pycache__/soo.cpython-38.pyc' -> './Source_Data/__pycache__/soo.cpython-38.pyc'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/__pycache__/voo.cpython-38.pyc' -> './Source_Data/__pycache__/voo.cpython-38.pyc'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/analyse_all_methods.py' -> './Source_Data/analyse_all_methods.py'\n",
      "'./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/Source_Data/test-LunarLander.png' -> './Source_Data/test-LunarLander.png'\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "run this section if you want to copy all files and folders from cloned folder\n",
    "(DANTE/SelfDriving_Virtual_Labs/Lunar_Landing) to current directory (/content/ or ./)\n",
    "\n",
    "So you can load all the FE simulation dataset, preTrained networks files without changing any paths\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "!cp -rv ./DANTE/SelfDriving_Virtual_Labs/Lunar_Landing/* ./\n",
    "\n",
    "print(\"============================================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kH9xmR42PI4",
    "outputId": "e19df848-cf7d-480c-dd74-33921a64ce28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "rm: remove write-protected regular file './DANTE/.git/objects/pack/pack-4ec4986ee6ae35abea5c73fceed27c15bf8c72e5.idx'? ^C\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
    "(after you have copied its contents to current directory)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "# delete original cloned folder\n",
    "!rm -r ./DANTE\n",
    "\n",
    "# delete cloned ipynb file\n",
    "!rm ./DANTE_VL_Lunar_Landing.ipynb\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWnbLz5CbZ5I"
   },
   "source": [
    "################################################################################\n",
    "> # **Install Dependencies**\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ofplf2sC3Pzy"
   },
   "outputs": [],
   "source": [
    "\n",
    "############ install compatible version of gymnasium and ###########\n",
    "\n",
    "# !pip install gymnasium\n",
    "\n",
    "# !pip install scipy\n",
    "\n",
    "# !pip install numpy\n",
    "\n",
    "# !pip install pandas\n",
    "\n",
    "# !pip install matplotlib\n",
    "\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# !pip install scikit-image\n",
    "\n",
    "# !pip install tqdm\n",
    "\n",
    "# !pip install seaborn\n",
    "\n",
    "# !pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSNSclkw3xQ2"
   },
   "source": [
    "################################################################################\n",
    "> # **Introduction**\n",
    "> The notebook is divided into 4 major parts :\n",
    "\n",
    "*   **Part I** : define the functions\n",
    "*   **Part II** : define the search algorithm\n",
    "*   **Part III** : optimization using the search algorithm\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-hgD8hm346Z"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - I**\n",
    "\n",
    "*   Define the functions\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "SSK0SF2J4aF6",
    "outputId": "5cb806ab-7768-4302-9f34-4cd7f6a8d6b9"
   },
   "outputs": [],
   "source": [
    "\n",
    "############################### Import libraries ###############################\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################### Set the objective ###############################\n",
    "\n",
    "class args:\n",
    "    def __init__(self, func = None, dims = 10, iters = 1000, method = None):\n",
    "        self.dims    = dims \n",
    "        self.func    = func\n",
    "        self.iterations =  iters\n",
    "        self.method  = method\n",
    "\n",
    "args = args(func ='LunarLander',            # specify the test function\n",
    "            dims = 100,                 # specify the problem dimensions\n",
    "            iters = 10000,              # specify the iterations to collect in the search\n",
    "            method = 'DANTE'           # specify the method to search\n",
    "            )\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory DANTE-LunarLander100 \n"
     ]
    }
   ],
   "source": [
    "############################### Define the functions ###############################\n",
    "\n",
    "class LunarLander:\n",
    "    \n",
    "    def __init__(self, dims=100, turn=1):\n",
    "        self.dims         = dims\n",
    "        self.lb           = 0 * np.ones(self.dims)\n",
    "        self.ub           = 3 * np.ones(self.dims)\n",
    "        self.counter      = 0\n",
    "        self.env          = gym.make(\"LunarLander-v3\")\n",
    "        \n",
    "        self.turn    = turn\n",
    "        \n",
    "   \n",
    "    def __call__(self, x):\n",
    "        x = np.array(x / self.turn).round(0) * self.turn\n",
    "        self.counter += 1\n",
    "        assert len(x) == self.dims\n",
    "        assert x.ndim == 1\n",
    "        assert np.all(x <= self.ub) and np.all(x >= self.lb)\n",
    "        \n",
    "        obs, info    = self.env.reset(seed=42)\n",
    "        # print(obs)\n",
    "        done1   = False\n",
    "        done2   = False\n",
    "        totalr = 0.\n",
    "        steps  = 0\n",
    "        \n",
    "        for i in range(self.dims):\n",
    "            action = round(x[i])\n",
    "            # print(action,'action')\n",
    "            obs, r, done1, done2, _ = self.env.step(action)\n",
    "            totalr += r\n",
    "            steps  += 1\n",
    "            if done1 or done2:\n",
    "                break\n",
    "\n",
    "        return totalr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class tracker:\n",
    "    def __init__(self, foldername):\n",
    "        self.counter   = 0\n",
    "        self.results   = []\n",
    "        self.curt_best = float(\"-inf\")\n",
    "        self.curt_best_x = None\n",
    "        self.foldername = foldername\n",
    "        try:\n",
    "            os.mkdir(foldername)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % foldername)\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s \" % foldername)\n",
    "        \n",
    "    def dump_trace(self):\n",
    "        # trace_path = self.foldername + '/result' + str(len( self.results) )\n",
    "        trace_path = self.foldername + '/result'\n",
    "        final_results_str = json.dumps(self.results)\n",
    "        with open(trace_path, \"a\") as f:\n",
    "            f.write(final_results_str + '\\n')\n",
    "            \n",
    "    def track(self, result, x = None, iters = 1000):\n",
    "        if result > self.curt_best:\n",
    "            self.curt_best = result\n",
    "            self.curt_best_x = x\n",
    "            print(\"\")\n",
    "            print(\"=\"*10)\n",
    "            print(\"iteration:\", self.counter, \"total samples:\", len(self.results) )\n",
    "            print(\"=\"*10)\n",
    "            print(\"current best f(x):\", self.curt_best)\n",
    "            print(\"current best x:\", np.around(self.curt_best_x, decimals=0))\n",
    "        self.results.append(self.curt_best)\n",
    "        self.counter += 1\n",
    "        if len(self.results) == iters:\n",
    "            self.dump_trace()\n",
    "        elif round(self.curt_best,5) == 0:\n",
    "            self.dump_trace()\n",
    "\n",
    "\n",
    "\n",
    "class Surrogate:\n",
    "    def __init__(self, dims=3, name = 'method', f=None, iters = None):\n",
    "        self.dims  = dims\n",
    "        self.name  = name\n",
    "        self.f     = f\n",
    "        self.counter = 0\n",
    "        self.tracker = tracker(name+str(dims))\n",
    "        self.iters = iters\n",
    "        self.turn = f.turn\n",
    "        self.lb   = f.lb\n",
    "        self.ub   = f.ub\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.array(x)\n",
    "        self.counter += 1\n",
    "        assert len(x) == self.dims\n",
    "        assert x.ndim == 1\n",
    "        result = self.f(x.reshape(self.dims))\n",
    "        \n",
    "        self.tracker.track( result, x, self.iters)\n",
    "                \n",
    "        return result \n",
    "\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "f = LunarLander(dims =args.dims)\n",
    "fx = Surrogate(dims =args.dims, name=args.method+'-LunarLander', f=f, iters = args.iterations)\n",
    "\n",
    "\n",
    "\n",
    "assert args.dims > 0\n",
    "assert f is not None\n",
    "assert args.iterations > 0\n",
    "\n",
    "lower = f.lb\n",
    "upper = f.ub\n",
    "\n",
    "bounds = []\n",
    "for idx in range(0, len(f.lb) ):\n",
    "    bounds.append( ( float(f.lb[idx]), float(f.ub[idx])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Rw1EcAXeEQMW"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################# End of Part I ################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxroUhC8EYBQ"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - II**\n",
    "\n",
    "*   Define the DANTE alghorithm\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8MjcwpM4Esp_"
   },
   "outputs": [],
   "source": [
    "################################# DANTE alghorithm ################################\n",
    "\n",
    "def create_new(tup, index, aaa, f):\n",
    "    flip = random.randint(0,5)\n",
    "    if flip ==0:\n",
    "        for i in range(int(f.dims/5)):\n",
    "          index_2 = random.randint(0, len(tup)-1)\n",
    "          tup[index_2] = np.random.choice(aaa)\n",
    "    elif flip ==1:\n",
    "        for i in range(int(f.dims/10)):\n",
    "          index_2 = random.randint(0, len(tup)-1)\n",
    "          tup[index_2] = np.random.choice(aaa)\n",
    "    elif flip ==2:\n",
    "        for i in range(int(f.dims/20)):\n",
    "          index_2 = random.randint(0, len(tup)-1)\n",
    "          tup[index_2] = np.random.choice(aaa)\n",
    "    elif flip ==3:\n",
    "        for i in range(int(f.dims/50)):\n",
    "          index_2 = random.randint(0, len(tup)-1)\n",
    "          tup[index_2] = np.random.choice(aaa)\n",
    "    elif flip ==4:\n",
    "        for i in range(int(f.dims/3)):\n",
    "          index_2 = random.randint(0, len(tup)-1)\n",
    "          tup[index_2] = np.random.choice(aaa)\n",
    "    else:\n",
    "        tup[index] = np.random.choice(aaa)\n",
    "    tup[index] = round(tup[index],5)\n",
    "    if tup[index] > f.ub[0]:\n",
    "        tup[index] = f.ub[0]\n",
    "    if tup[index] < f.lb[0]:\n",
    "        tup[index] = f.lb[0]\n",
    "    return tup\n",
    "\n",
    "################################# DANTE alghorithm ################################\n",
    "\n",
    "#######################################################\n",
    "class DANTE:\n",
    "    def __init__(self, exploration_weight=None, f = None, name = None):\n",
    "        self.Q = defaultdict(int)  # total reward of each node\n",
    "        self.Qa = dict.fromkeys(list(range(20)), 0)\n",
    "        self.Na = dict.fromkeys(list(range(20)), 1)\n",
    "        self.N = defaultdict(int)  # total visit count for each node\n",
    "        self.children = dict()  # children of each node\n",
    "        self.exploration_weight = exploration_weight\n",
    "        \n",
    "        print(self.exploration_weight)\n",
    "        self.f = f\n",
    "        self.name = name\n",
    "\n",
    "    def choose(self, node):\n",
    "        \"Choose the best successor of node.\"\n",
    "        if node.is_terminal():\n",
    "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
    "\n",
    "        if node not in self.children:\n",
    "            print('not seen before, randomly sampled!')\n",
    "            return node.find_random_child()\n",
    "\n",
    "        def score(n):\n",
    "            if self.N[n] == 0:\n",
    "               return '-inf'  # avoid unseen moves\n",
    "            return self.Q[n] / self.N[n]  # average reward\n",
    "        def evaluate(n):\n",
    "            return n.value  # average reward\n",
    "        log_N_vertex = math.log(self.N[node])\n",
    "        def uct(n):\n",
    "            \"Upper confidence bound for trees\"\n",
    "            uct_value = n.value + self.exploration_weight * math.sqrt(\n",
    "                log_N_vertex / (self.N[n]+1))\n",
    "            return uct_value\n",
    "\n",
    "        media_node = max(self.children[node], key=uct)\n",
    "\n",
    "        if uct(media_node) > uct(node):\n",
    "            return media_node\n",
    "        return node\n",
    "\n",
    "    def do_rollout(self, node):\n",
    "        \"Make the tree one layer better. (Train for one iteration.)\"\n",
    "        path = self._select(node)\n",
    "        leaf = path[-1]\n",
    "        self._expand(leaf)\n",
    "        reward = self._simulate(leaf)\n",
    "        self._backpropagate(path, reward)\n",
    "\n",
    "    def _select(self, node):\n",
    "        \"Find an unexplored descendent of `node`\"\n",
    "        path = []\n",
    "        count = 0\n",
    "        while True:\n",
    "            path.append(node)\n",
    "            if node not in self.children or not self.children[node]:\n",
    "                # node is either unexplored or terminal\n",
    "              return path\n",
    "            unexplored = self.children[node] - self.children.keys()\n",
    "            def evaluate(n):\n",
    "              return n.value\n",
    "            if count == 5:\n",
    "                return path\n",
    "          \n",
    "            if unexplored:\n",
    "              path.append(max(unexplored, key=evaluate))#\n",
    "              return path\n",
    "            node = self._uct_select(node)  # descend a layer deeper\n",
    "            count+=1\n",
    "\n",
    "    def _expand(self, node):\n",
    "        \"Update the `children` dict with the children of `node`\"\n",
    "        # if node in self.children:\n",
    "        #     return  # already expanded\n",
    "        action = [p for p in range(0, len(node.tup))]\n",
    "        # action = [p for p in range(0, int(len(node.tup)/2))]\n",
    "        self.children[node] = node.find_children(action, self.f)\n",
    "\n",
    "    def _simulate(self, node):\n",
    "        \"Returns the reward for a random simulation (to completion) of `node`\"\n",
    "        # reward = node.reward()\n",
    "        reward = node.reward(self.f)\n",
    "        return reward\n",
    "\n",
    "    def _backpropagate(self, path, reward):\n",
    "        \"Send the reward back up to the ancestors of the leaf\"\n",
    "        for node in reversed(path):\n",
    "          self.N[node] += 1\n",
    "          self.Q[node] += reward\n",
    "\n",
    "    def _uct_select(self, node):\n",
    "        \"Select a child of node, balancing exploration & exploitation\"\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "        log_N_vertex = math.log(self.N[node])\n",
    "\n",
    "        def uct(n):\n",
    "            \"Upper confidence bound for trees\"\n",
    "            uct_value = n.value + self.exploration_weight * math.sqrt(\n",
    "                log_N_vertex / (self.N[n]+1))\n",
    "            return uct_value\n",
    "        uct_node = max(self.children[node], key=uct)\n",
    "        return uct_node\n",
    "\n",
    "class Node(ABC):\n",
    "    \"\"\"\n",
    "    A representation of a single board state.\n",
    "    DOTS works by constructing a tree of these Nodes.\n",
    "    Could be e.g. a chess or checkers board state.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def find_children(self):\n",
    "        \"All possible successors of this board state\"\n",
    "        return set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_random_child(self):\n",
    "        \"Random successor of this board state (for more efficient simulation)\"\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self):\n",
    "        \"Returns True if the node has no children\"\n",
    "        return True\n",
    "\n",
    "    @abstractmethod\n",
    "    def reward(self):\n",
    "        \"Assumes `self` is terminal node. 1=win, 0=loss, .5=tie, etc\"\n",
    "        return 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def __hash__(self):\n",
    "        \"Nodes must be hashable\"\n",
    "        return 123456789\n",
    "\n",
    "    @abstractmethod\n",
    "    def __eq__(node1, node2):\n",
    "        \"Nodes must be comparable\"\n",
    "        return True\n",
    "\n",
    "\n",
    "_OT = namedtuple(\"opt_task\", \"tup value terminal\")\n",
    "\n",
    "# Inheriting from a namedtuple is convenient because it makes the class\n",
    "# immutable and predefines __init__, __repr__, __hash__, __eq__, and others\n",
    "class opt_task(_OT, Node):\n",
    "        \n",
    "    def find_children(board,action,f):\n",
    "        if board.terminal:  # If the game is finished then no moves can be made\n",
    "            return set()\n",
    "        return {\n",
    "            board.make_move(i, f) for i in action}\n",
    "\n",
    "    def find_random_child(board):\n",
    "        pass\n",
    "\n",
    "    def find_uct_child(board, action):\n",
    "        pass\n",
    "\n",
    "    def reward(board,f):\n",
    "        return board.value\n",
    "\n",
    "    def is_terminal(board):\n",
    "        return board.terminal\n",
    "\n",
    "    def make_move(board, index, f):\n",
    "        tup = list(board.tup)\n",
    "        aaa = np.arange(f.lb[0], f.ub[0] + f.turn, f.turn).round(5)\n",
    "        \n",
    "        tup = create_new(tup, index, aaa, f)\n",
    "            \n",
    "        value = round(f(np.array(tup)),10)\n",
    "        is_terminal = False\n",
    "        return opt_task(tuple(tup), value, is_terminal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kfgSnreJFApR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "################################ End of Part II ################################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fWnmV2NJdGs"
   },
   "source": [
    "################################################################################\n",
    "> # **Part - III**\n",
    "\n",
    "*   Optimization using DANTE\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "5JNNKUBpK30Y",
    "outputId": "8a42ed4e-ce33-43fd-8629-f98960a04887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========\n",
      "iteration: 0 total samples: 0\n",
      "==========\n",
      "current best f(x): -64.04226128209184\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 3. 1. 0. 3. 0. 1. 3. 2. 1. 3. 2. 2.\n",
      " 2. 1. 3. 0. 2. 3. 3. 1. 2. 3. 3. 2. 3. 0. 3. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 3. 0. 3. 0. 0. 3. 0. 1. 1. 1. 2. 1. 0. 2. 3. 0. 1. 1. 3. 2.\n",
      " 2. 3. 3. 0. 0. 0. 2. 0. 3. 1. 0. 0. 0. 2. 0. 1. 3. 2. 1. 1. 0. 3. 2. 1.\n",
      " 1. 3. 1. 3.]\n",
      "-32.02113064105\n",
      "\n",
      "==========\n",
      "iteration: 3 total samples: 3\n",
      "==========\n",
      "current best f(x): -37.826737749408906\n",
      "current best x: [1. 0. 0. 1. 1. 2. 1. 2. 0. 3. 2. 0. 3. 0. 0. 3. 0. 1. 3. 0. 3. 3. 2. 2.\n",
      " 2. 0. 3. 0. 2. 3. 3. 1. 2. 0. 3. 2. 3. 0. 3. 0. 1. 1. 2. 2. 1. 1. 1. 0.\n",
      " 3. 1. 2. 3. 3. 3. 0. 0. 0. 0. 0. 1. 1. 1. 2. 1. 0. 2. 3. 0. 1. 0. 3. 1.\n",
      " 2. 3. 3. 0. 0. 1. 2. 0. 3. 1. 0. 0. 0. 2. 0. 1. 3. 0. 1. 0. 0. 2. 2. 1.\n",
      " 1. 2. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 4 total samples: 4\n",
      "==========\n",
      "current best f(x): -27.123081541503918\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 3. 1. 0. 1. 0. 1. 3. 2. 1. 3. 2. 2.\n",
      " 2. 1. 3. 0. 2. 3. 3. 1. 2. 3. 3. 2. 3. 0. 3. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 3. 0. 3. 0. 0. 3. 0. 1. 1. 1. 2. 1. 0. 2. 3. 0. 2. 1. 3. 2.\n",
      " 2. 3. 3. 0. 0. 0. 2. 0. 3. 1. 0. 0. 0. 2. 0. 1. 3. 2. 1. 1. 0. 3. 2. 1.\n",
      " 1. 3. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 17 total samples: 17\n",
      "==========\n",
      "current best f(x): -26.83772111147133\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 3. 1. 0. 0. 0. 1. 3. 2. 1. 3. 2. 2.\n",
      " 2. 1. 3. 0. 2. 3. 3. 1. 2. 3. 3. 2. 3. 0. 3. 1. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 2. 3. 0. 3. 0. 3. 0. 0. 1. 1. 1. 2. 1. 0. 2. 2. 0. 1. 1. 3. 2.\n",
      " 2. 3. 3. 0. 0. 0. 2. 0. 3. 1. 0. 3. 0. 3. 0. 1. 3. 2. 1. 0. 0. 3. 2. 1.\n",
      " 1. 3. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 55 total samples: 55\n",
      "==========\n",
      "current best f(x): -25.68491428617003\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 3. 1. 0. 3. 0. 1. 3. 2. 0. 3. 2. 2.\n",
      " 2. 1. 3. 0. 2. 3. 1. 0. 2. 0. 0. 2. 1. 0. 3. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 3. 0. 3. 0. 1. 3. 0. 1. 3. 1. 3. 1. 0. 2. 2. 0. 1. 1. 0. 2.\n",
      " 2. 3. 3. 0. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 0. 1. 3. 2. 1. 3. 0. 3. 2. 1.\n",
      " 1. 3. 3. 1.]\n",
      "\n",
      "==========\n",
      "iteration: 74 total samples: 74\n",
      "==========\n",
      "current best f(x): -23.60083923329971\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 0. 1. 0. 3. 0. 1. 3. 2. 1. 3. 2. 1.\n",
      " 3. 1. 3. 0. 2. 2. 3. 1. 2. 3. 3. 2. 0. 2. 3. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 3. 0. 3. 0. 0. 3. 0. 1. 1. 3. 2. 1. 0. 2. 3. 1. 3. 1. 3. 2.\n",
      " 2. 3. 3. 2. 1. 0. 2. 0. 3. 1. 0. 0. 0. 2. 0. 1. 2. 2. 1. 1. 3. 3. 0. 1.\n",
      " 1. 2. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 75 total samples: 75\n",
      "==========\n",
      "current best f(x): -20.67544929919603\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 3. 1. 0. 3. 0. 1. 3. 2. 1. 3. 1. 2.\n",
      " 2. 1. 3. 0. 2. 3. 3. 1. 2. 3. 3. 2. 3. 0. 1. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 3. 0. 3. 0. 0. 3. 0. 1. 1. 1. 2. 1. 0. 2. 3. 0. 0. 1. 3. 2.\n",
      " 2. 3. 3. 0. 0. 0. 2. 0. 3. 1. 0. 0. 0. 2. 0. 1. 3. 2. 0. 1. 0. 3. 2. 1.\n",
      " 1. 3. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 91 total samples: 91\n",
      "==========\n",
      "current best f(x): -20.60197372374393\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 3. 1. 0. 3. 2. 1. 3. 2. 1. 3. 2. 2.\n",
      " 3. 1. 1. 0. 2. 3. 3. 1. 2. 3. 1. 2. 0. 0. 3. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 0. 3. 0. 2. 3. 0. 3. 1. 1. 2. 3. 0. 2. 3. 0. 1. 3. 1. 2.\n",
      " 2. 3. 3. 0. 0. 0. 2. 2. 3. 1. 0. 0. 2. 2. 0. 1. 3. 2. 1. 1. 0. 3. 2. 1.\n",
      " 2. 2. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 101 total samples: 101\n",
      "==========\n",
      "current best f(x): -14.06382263276761\n",
      "current best x: [0. 1. 0. 2. 1. 2. 1. 2. 0. 3. 2. 3. 3. 1. 0. 3. 2. 1. 3. 2. 1. 3. 2. 2.\n",
      " 3. 1. 1. 0. 2. 3. 3. 1. 2. 3. 1. 2. 0. 0. 3. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 0. 3. 0. 2. 3. 0. 3. 1. 1. 2. 3. 0. 2. 3. 0. 1. 3. 1. 2.\n",
      " 2. 3. 3. 0. 0. 0. 2. 2. 3. 1. 0. 0. 2. 2. 0. 1. 3. 2. 1. 1. 2. 3. 2. 1.\n",
      " 2. 2. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 106 total samples: 106\n",
      "==========\n",
      "current best f(x): -7.234097758675653\n",
      "current best x: [0. 0. 0. 2. 1. 0. 1. 2. 0. 3. 2. 3. 3. 1. 0. 3. 2. 1. 3. 2. 1. 3. 2. 2.\n",
      " 3. 1. 1. 0. 2. 3. 3. 1. 2. 3. 1. 2. 0. 0. 3. 0. 1. 1. 2. 2. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 0. 3. 0. 2. 3. 0. 3. 1. 1. 2. 3. 0. 2. 3. 0. 1. 3. 1. 2.\n",
      " 2. 3. 3. 0. 0. 0. 2. 2. 3. 1. 0. 0. 2. 2. 0. 1. 3. 2. 1. 1. 0. 3. 2. 1.\n",
      " 2. 2. 1. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 156 total samples: 156\n",
      "==========\n",
      "current best f(x): -1.547391815341249\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 1. 0. 3. 3. 1. 1. 3. 2. 1. 3. 2. 1. 2. 2. 2.\n",
      " 3. 1. 1. 0. 3. 3. 3. 3. 2. 3. 1. 2. 0. 0. 3. 2. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 0. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 0. 3. 3. 3. 1. 3. 1. 2.\n",
      " 2. 3. 3. 1. 0. 0. 2. 2. 3. 0. 0. 0. 2. 2. 0. 1. 3. 2. 1. 1. 0. 3. 2. 2.\n",
      " 2. 2. 1. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 201 total samples: 201\n",
      "==========\n",
      "current best f(x): -1.237557423277507\n",
      "current best x: [0. 0. 0. 2. 1. 2. 1. 2. 0. 2. 0. 3. 3. 1. 1. 3. 1. 1. 3. 2. 1. 2. 2. 2.\n",
      " 3. 1. 1. 0. 3. 3. 3. 3. 2. 3. 1. 2. 0. 0. 3. 1. 2. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 3. 3. 3. 0. 3. 2. 2. 2. 0. 0. 1. 1. 2. 3. 0. 3. 1. 3. 1. 3. 3. 2.\n",
      " 2. 1. 3. 1. 0. 0. 1. 2. 3. 1. 1. 0. 2. 2. 0. 3. 3. 2. 1. 3. 2. 3. 2. 2.\n",
      " 2. 2. 1. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 204 total samples: 204\n",
      "==========\n",
      "current best f(x): 20.102443610741446\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 1. 0. 3. 1. 1. 3. 0. 1. 3. 1. 1. 2. 2. 2.\n",
      " 3. 1. 0. 0. 3. 3. 3. 3. 2. 0. 1. 2. 0. 0. 0. 2. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 3. 3. 3. 3. 3. 1. 2.\n",
      " 2. 1. 1. 1. 0. 1. 3. 2. 3. 0. 0. 1. 2. 2. 0. 1. 3. 2. 1. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 326 total samples: 326\n",
      "==========\n",
      "current best f(x): 25.638445547058783\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 3. 0. 3. 1. 1. 2. 0. 3. 3. 1. 1. 2. 2. 2.\n",
      " 3. 1. 0. 0. 3. 3. 3. 0. 2. 0. 1. 3. 0. 0. 0. 2. 2. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 2. 3. 2. 2. 2. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 3. 3. 3. 3. 3. 0. 2.\n",
      " 2. 1. 1. 2. 1. 3. 3. 2. 3. 0. 0. 1. 2. 2. 0. 1. 3. 2. 1. 1. 0. 3. 2. 2.\n",
      " 1. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 327 total samples: 327\n",
      "==========\n",
      "current best f(x): 37.474294232429855\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 1. 0. 3. 1. 1. 3. 0. 1. 3. 1. 1. 2. 2. 2.\n",
      " 3. 1. 0. 0. 3. 3. 3. 3. 2. 0. 1. 2. 0. 0. 0. 2. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 3. 3. 3. 3. 3. 1. 2.\n",
      " 2. 1. 1. 1. 0. 1. 3. 2. 3. 0. 0. 1. 2. 2. 0. 1. 1. 2. 1. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 332 total samples: 332\n",
      "==========\n",
      "current best f(x): 50.29988331655573\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 1. 0. 3. 1. 1. 3. 0. 1. 3. 1. 1. 2. 2. 2.\n",
      " 3. 1. 0. 0. 3. 3. 3. 3. 2. 0. 1. 1. 0. 0. 0. 3. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 2. 3. 3. 0. 3. 1. 2.\n",
      " 2. 1. 1. 1. 0. 1. 3. 2. 3. 0. 0. 1. 2. 2. 0. 1. 3. 2. 1. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 365 total samples: 365\n",
      "==========\n",
      "current best f(x): 67.41261042287086\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 1. 0. 3. 1. 1. 3. 0. 1. 3. 1. 1. 2. 3. 2.\n",
      " 3. 1. 0. 0. 3. 3. 3. 3. 2. 0. 1. 2. 0. 0. 0. 2. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 3. 3. 3. 3. 3. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 0. 1. 2. 2. 0. 1. 2. 2. 2. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 429 total samples: 429\n",
      "==========\n",
      "current best f(x): 68.44892757368379\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 1. 1. 0. 3. 1. 1. 3. 0. 1. 3. 1. 1. 2. 3. 2.\n",
      " 3. 1. 0. 0. 3. 3. 3. 3. 2. 0. 1. 2. 0. 0. 0. 2. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 3. 3. 3. 3. 3. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 0. 1. 2. 2. 0. 1. 2. 2. 2. 1. 1. 3. 2. 0.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 471 total samples: 471\n",
      "==========\n",
      "current best f(x): 77.73245252555179\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 0. 1. 0. 3. 1. 1. 3. 0. 1. 3. 2. 1. 1. 3. 0.\n",
      " 3. 1. 0. 2. 3. 3. 3. 3. 2. 0. 1. 0. 0. 0. 0. 2. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 1. 3. 3. 3. 3. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 2. 1. 2. 2. 0. 1. 2. 2. 2. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 515 total samples: 515\n",
      "==========\n",
      "current best f(x): 79.22841486070494\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 0. 1. 0. 3. 1. 1. 3. 0. 1. 3. 2. 1. 1. 3. 0.\n",
      " 3. 1. 0. 2. 3. 3. 3. 3. 2. 0. 1. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 1. 3. 3. 3. 0. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 2. 1. 2. 2. 0. 1. 2. 2. 2. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 652 total samples: 652\n",
      "==========\n",
      "current best f(x): 80.64701150134172\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 0. 1. 0. 3. 1. 1. 3. 0. 1. 3. 2. 1. 1. 3. 0.\n",
      " 3. 1. 0. 2. 3. 3. 3. 3. 2. 0. 1. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 1. 3. 3. 3. 0. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 2. 1. 2. 2. 0. 1. 2. 2. 2. 1. 1. 0. 2. 2.\n",
      " 2. 2. 3. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 680 total samples: 680\n",
      "==========\n",
      "current best f(x): 89.21787447127387\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 0. 1. 0. 3. 1. 1. 3. 3. 1. 3. 2. 1. 1. 3. 0.\n",
      " 3. 1. 0. 1. 3. 3. 3. 3. 2. 0. 1. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 1. 3. 3. 3. 0. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 2. 1. 2. 2. 0. 1. 2. 2. 2. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 682 total samples: 682\n",
      "==========\n",
      "current best f(x): 108.68299322833036\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 0. 1. 0. 3. 1. 1. 3. 0. 1. 3. 2. 1. 1. 3. 0.\n",
      " 3. 1. 0. 2. 3. 3. 3. 3. 2. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 1. 3. 3. 3. 0. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 2. 1. 2. 3. 0. 1. 2. 2. 2. 1. 1. 3. 2. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 781 total samples: 781\n",
      "==========\n",
      "current best f(x): 110.48605500351009\n",
      "current best x: [0. 0. 0. 0. 1. 1. 1. 2. 0. 0. 1. 0. 3. 1. 1. 3. 0. 1. 3. 2. 1. 1. 3. 0.\n",
      " 3. 1. 0. 2. 3. 3. 3. 3. 2. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 2. 3. 2. 2. 3. 2. 2. 3. 0. 3. 1. 1. 2. 3. 2. 1. 3. 3. 3. 0. 1. 1.\n",
      " 2. 1. 1. 1. 0. 1. 1. 2. 3. 0. 2. 1. 2. 3. 0. 1. 2. 2. 2. 1. 1. 3. 3. 2.\n",
      " 2. 2. 3. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 4025 total samples: 4025\n",
      "==========\n",
      "current best f(x): 113.14107030097341\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 1. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 3. 0. 0. 0. 3. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 2. 0. 0. 3. 2. 1. 2. 0. 0. 0. 0. 1. 0. 0. 1. 1. 3. 0.\n",
      " 1. 1. 2. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 4860 total samples: 4860\n",
      "==========\n",
      "current best f(x): 115.57377065095952\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 1. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 3. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 0. 0. 0. 1. 3. 0. 1. 1. 3. 3.\n",
      " 1. 1. 2. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 4994 total samples: 4994\n",
      "==========\n",
      "current best f(x): 116.24260823310016\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 1. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 3. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 0. 0. 0. 1. 3. 0. 1. 0. 3. 3.\n",
      " 1. 1. 2. 3.]\n",
      "\n",
      "==========\n",
      "iteration: 5000 total samples: 5000\n",
      "==========\n",
      "current best f(x): 120.50830996357534\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 1. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 3. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 0. 0. 0. 1. 3. 0. 1. 1. 3. 3.\n",
      " 1. 1. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 5290 total samples: 5290\n",
      "==========\n",
      "current best f(x): 129.55314958724625\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 0. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 1. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 1. 0. 0. 1. 3. 0. 1. 1. 3. 3.\n",
      " 1. 1. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 5397 total samples: 5397\n",
      "==========\n",
      "current best f(x): 131.83940175730146\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 0. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 1. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 1. 0. 0. 1. 3. 0. 1. 1. 3. 3.\n",
      " 3. 1. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 5498 total samples: 5498\n",
      "==========\n",
      "current best f(x): 133.44496359368364\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 0. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 1. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 1. 0. 0. 1. 3. 0. 1. 1. 3. 3.\n",
      " 3. 3. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 5790 total samples: 5790\n",
      "==========\n",
      "current best f(x): 136.57024198833344\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 0. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 1. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 1. 0. 0. 3. 3. 0. 1. 1. 2. 3.\n",
      " 3. 1. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 5898 total samples: 5898\n",
      "==========\n",
      "current best f(x): 137.38216741670507\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 0. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 1. 3. 2. 1. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 3. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 1. 0. 0. 3. 3. 0. 1. 1. 2. 3.\n",
      " 3. 0. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 6017 total samples: 6017\n",
      "==========\n",
      "current best f(x): 137.8662706181323\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 1. 0. 3. 1. 3. 2. 0. 1. 3. 0.\n",
      " 0. 1. 2. 2. 3. 3. 3. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 1. 3. 1. 0. 0. 1. 2.\n",
      " 3. 3. 2. 1. 0. 0. 0. 0. 1. 3. 2. 3. 1. 0. 3. 3. 2. 2. 3. 0. 3. 1. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 1. 0. 0. 1. 3. 0. 3. 1. 2. 3.\n",
      " 3. 3. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 6294 total samples: 6294\n",
      "==========\n",
      "current best f(x): 137.9387685568123\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 0. 0. 3. 1. 3. 2. 1. 1. 3. 3.\n",
      " 0. 1. 2. 2. 3. 3. 0. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 0. 3. 1. 0. 0. 1. 2.\n",
      " 0. 3. 2. 1. 0. 0. 0. 0. 1. 3. 2. 0. 1. 0. 3. 3. 2. 2. 3. 0. 3. 0. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 1. 0. 0. 1. 3. 0. 3. 3. 2. 3.\n",
      " 3. 3. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 7531 total samples: 7531\n",
      "==========\n",
      "current best f(x): 140.8633603881816\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 1. 0. 3. 1. 0. 0. 3. 1. 3. 0. 1. 1. 3. 3.\n",
      " 0. 1. 2. 2. 3. 3. 2. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 1. 3. 1. 0. 0. 1. 2.\n",
      " 0. 3. 2. 1. 0. 0. 3. 0. 1. 3. 2. 2. 1. 0. 3. 3. 2. 0. 3. 0. 3. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 3. 0. 0. 2. 3. 1. 3. 2. 2. 3.\n",
      " 3. 3. 2. 1.]\n",
      "\n",
      "==========\n",
      "iteration: 9096 total samples: 9096\n",
      "==========\n",
      "current best f(x): 140.90522888984816\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 3. 0. 1. 1. 0. 0. 3. 1. 3. 0. 1. 1. 3. 3.\n",
      " 0. 1. 2. 2. 3. 3. 2. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 1. 3. 1. 0. 0. 1. 0.\n",
      " 0. 3. 2. 1. 0. 0. 3. 0. 1. 3. 2. 2. 1. 0. 3. 3. 2. 0. 3. 0. 3. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 0. 3. 0. 0. 2. 3. 2. 3. 2. 2. 3.\n",
      " 3. 0. 2. 1.]\n",
      "\n",
      "==========\n",
      "iteration: 9386 total samples: 9386\n",
      "==========\n",
      "current best f(x): 143.54729479500736\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 3. 0. 1. 1. 0. 0. 3. 1. 3. 0. 1. 1. 3. 3.\n",
      " 0. 1. 2. 2. 3. 3. 2. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 1. 3. 1. 0. 0. 1. 0.\n",
      " 0. 3. 2. 1. 0. 1. 3. 0. 1. 3. 2. 2. 1. 0. 3. 3. 2. 0. 3. 0. 3. 1. 1. 3.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 3. 3. 0. 0. 2. 3. 2. 3. 2. 2. 3.\n",
      " 3. 0. 2. 1.]\n",
      "\n",
      "==========\n",
      "iteration: 9634 total samples: 9634\n",
      "==========\n",
      "current best f(x): 145.92744240764432\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 3. 0. 1. 1. 0. 0. 3. 1. 3. 0. 1. 1. 3. 3.\n",
      " 0. 1. 2. 2. 3. 3. 2. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 1. 3. 1. 0. 0. 1. 0.\n",
      " 0. 3. 2. 1. 0. 1. 3. 0. 1. 3. 2. 2. 1. 3. 3. 3. 2. 1. 3. 0. 3. 1. 1. 3.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 3. 3. 0. 0. 2. 3. 2. 3. 2. 2. 3.\n",
      " 3. 0. 2. 2.]\n",
      "\n",
      "==========\n",
      "iteration: 9939 total samples: 9939\n",
      "==========\n",
      "current best f(x): 151.81058838052232\n",
      "current best x: [1. 1. 0. 1. 0. 1. 1. 0. 3. 1. 3. 0. 1. 1. 0. 0. 3. 1. 3. 0. 1. 1. 3. 3.\n",
      " 0. 1. 2. 2. 3. 3. 2. 2. 0. 0. 1. 0. 0. 0. 0. 1. 2. 1. 3. 1. 0. 0. 1. 0.\n",
      " 0. 3. 3. 1. 0. 1. 3. 0. 1. 3. 2. 2. 1. 3. 3. 3. 2. 1. 3. 0. 1. 1. 1. 3.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 1. 2. 3. 3. 0. 0. 2. 3. 2. 2. 2. 0. 3.\n",
      " 3. 0. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "################################ Optimization using DANTE ###############################\n",
    "\n",
    "aaa = np.arange(f.lb[0], f.ub[0] + f.turn, f.turn)\n",
    "init_X = np.random.choice(aaa,size=(args.dims))\n",
    "init_y = round(fx(init_X),10)\n",
    "\n",
    "ratio = 0.5\n",
    "\n",
    "exp_weight = ratio * init_y\n",
    "\n",
    "board_ubt = opt_task(tup=tuple(init_X), value=init_y, terminal=False)\n",
    "tree_ubt = DANTE(exploration_weight=exp_weight, f=fx, name=args.func)\n",
    "for i in range(args.iterations // args.dims):\n",
    "    tree_ubt.do_rollout(board_ubt)\n",
    "    board_ubt = tree_ubt.choose(board_ubt)\n",
    "    fy = f(np.array(board_ubt.tup))\n",
    "    tree_ubt.exploration_weight = ratio * round(fy,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DANTE-LunarLander100 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################ Visualization ###############################\n",
    "\n",
    "\n",
    "def process(name):\n",
    "    f = open('./'+name+'/result')\n",
    "    yourList = f.readlines()\n",
    "    yourList2=[]\n",
    "    max_len = 0\n",
    "    for i in yourList:\n",
    "        i=i.strip('[')\n",
    "        i=i.strip(']\\n')\n",
    "        i = [item.strip() for item in i.split(',')]\n",
    "        yourList2.append(i)\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    yourList3 = []\n",
    "    samples = []\n",
    "    for i in yourList2:\n",
    "        ii = np.array(i).astype(float)\n",
    "        samples.append(len(ii))\n",
    "        if len(ii) < max_len:\n",
    "            ii = np.concatenate((ii, np.zeros(max_len-len(ii))))\n",
    "        yourList3.append(ii)\n",
    "    yourList3 = np.array(yourList3)\n",
    "    mean = np.mean(yourList3, axis=0)\n",
    "    std = np.std(yourList3, axis=0)\n",
    "    print(name,len(yourList2))\n",
    "    return mean, std\n",
    "\n",
    "labels = ['DANTE']\n",
    "labels1 = ['DANTE']\n",
    "colors  = ['#FAB49B', ]\n",
    "\n",
    "funcs = args.func\n",
    "dims = args.dims\n",
    "plt.figure()\n",
    "for l,i in enumerate(labels):\n",
    "    try:\n",
    "        mean, std = process(i + '-' + funcs + str(dims))\n",
    "        plt.plot(np.arange(len(mean)), mean, '-', label = labels1[l], color = colors[l])\n",
    "        plt.fill_between(np.arange(len(mean)), mean -std, mean + std, alpha=0.2, facecolor=colors[l])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "plt.xlabel('Number of data acquisition')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(funcs +'-'+str(dims)+'d')\n",
    "\n",
    "plt.legend(prop={'size':10})\n",
    "#plt.ylim(-300,180)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MbB05NiORBGV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "################################ End of Part III ################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUOloGCFtCoA"
   },
   "source": [
    "################################################################################\n",
    "\n",
    "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
